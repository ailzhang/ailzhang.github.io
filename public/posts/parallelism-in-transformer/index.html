<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Parallelism in Transformer | Ailing's Blog</title><meta name=keywords content><meta name=description content="Visualizing the Distributed Transformer: An &ldquo;Accounting&rdquo; View of 4D Parallelism
Motivation: Simplicity buried in Abstractions
I&rsquo;ve always loved the &ldquo;Transformer Accounting&rdquo; diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.
But as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the &lsquo;alphabet soup&rsquo; of 4D parallelism (DP, TP, SP, CP, EP)."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/parallelism-in-transformer/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/parallelism-in-transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/parallelism-in-transformer/"><meta property="og:site_name" content="Ailing's Blog"><meta property="og:title" content="Parallelism in Transformer"><meta property="og:description" content="Visualizing the Distributed Transformer: An “Accounting” View of 4D Parallelism Motivation: Simplicity buried in Abstractions I’ve always loved the “Transformer Accounting” diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.
But as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the ‘alphabet soup’ of 4D parallelism (DP, TP, SP, CP, EP)."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-19T22:12:33-08:00"><meta property="article:modified_time" content="2026-01-19T22:12:33-08:00"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Parallelism in Transformer"><meta name=twitter:description content="Visualizing the Distributed Transformer: An &ldquo;Accounting&rdquo; View of 4D Parallelism
Motivation: Simplicity buried in Abstractions
I&rsquo;ve always loved the &ldquo;Transformer Accounting&rdquo; diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.
But as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the &lsquo;alphabet soup&rsquo; of 4D parallelism (DP, TP, SP, CP, EP)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Parallelism in Transformer","item":"http://localhost:1313/posts/parallelism-in-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Parallelism in Transformer","name":"Parallelism in Transformer","description":"Visualizing the Distributed Transformer: An \u0026ldquo;Accounting\u0026rdquo; View of 4D Parallelism Motivation: Simplicity buried in Abstractions I\u0026rsquo;ve always loved the \u0026ldquo;Transformer Accounting\u0026rdquo; diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.\nBut as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the \u0026lsquo;alphabet soup\u0026rsquo; of 4D parallelism (DP, TP, SP, CP, EP).\n","keywords":[],"articleBody":"Visualizing the Distributed Transformer: An “Accounting” View of 4D Parallelism Motivation: Simplicity buried in Abstractions I’ve always loved the “Transformer Accounting” diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.\nBut as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the ‘alphabet soup’ of 4D parallelism (DP, TP, SP, CP, EP).\nHere is the irony: The core ideas behind these parallelisms are actually fundamentally easy. Conceptually, we are just slicing tensors. But in practice, this simplicity gets buried under layers of dense abstractions and “spaghetti” implementations in production codebases.\nI created the diagrams below to cut through that noise. I want to restore that intuition by visually mapping the dance between Compute and Collectives. The goal is to show exactly how the compute is sharded and when the synchronization happens—stripping away the implementation complexity to reveal the simple logic underneath.\nHow to Read These Diagrams To interpret the figures below, think of them as a map of what is happening on a single GPU.\n1. The Legends To navigate the maps, you need to know the symbols.\nTensor Dimensions (The Global Object)\nSymbol Definition Note B Batch Size Global batch size across all devices. S Sequence Length Full sequence length (e.g., 4096, 128k). D Model Hidden Dimension The width of the residual stream. V Vocabulary Size Total size of the tokenizer vocabulary. F Feed-Forward Dimension Expansion dimension in MLP (usually 4*D). E Number of Experts Total experts in MoE layer. C Capacity Max tokens per expert (MoE specific). The Mesh (The Slicing Strategy)\nThese symbols represent the size of the process group used to shard a specific dimension.\nSymbol Definition Shards What? tp Tensor Parallel Weights ($D$ or $F$) in Linear Layers sp Sequence Parallel Activations ($S$) in Element-wise Ops cp Context Parallel Sequence ($S$) in Attention (QKV) ep Expert Parallel Experts ($E$) in MoE Layers vp Vocab Parallel Vocabulary ($V$) in Embeddings/Loss dp Data Parallel Batch ($B$) 2. How to Read the Maps (Visual Language) To interpret the figures below, imagine you are sitting inside one single GPU.\nGrey Boxes (Your Slice): These represent the data currently sitting in your GPU’s memory. The Notation X/y: This tells you how the global tensor is cut. It means “Take global dimension X and slice it into y parts.” Example: Reading the “Local Shape”\nYou will frequently see this specific shape entering and exiting the major blocks:\n[B/dp, S/(cp*sp), D]\nRead this literally from left to right:\nB/dp: “I only hold a fraction of the Batch.” (sliced by Data Parallelism) S/(cp*sp): “I only hold a tiny fragment of the Sequence.” (sliced by both Context and Sequence Parallelism) D: “I hold the full Hidden Model vector.” Because you hold the full model vector D, you can perform element-wise operations (like LayerNorm or Residual Adds) locally without talking to anyone else.\nThe Visual Walkthrough Part I: Embeddings \u0026 The “ReduceScatter” Trick This layer sets the stage for the rest of the model.\nInput: We start with tokens sharded by S/(cp*sp). Lookup: The embedding weights are sharded across the Vocab dimension (V/vp). This means a local lookup produces partial vectors (many are zeros). The Optimization: The naive approach would be to AllReduce (to sum the partial embeddings) and then Scatter (to shard them for Sequence Parallelism). Instead, we use a ReduceScatter. We sum the partial embeddings from the Vocab Parallel lookup and immediately scatter them into the Sequence Parallel dimension. This cuts communication overhead significantly right at the start. Part II: Attention (TP + SP + CP) Here we see the complex interplay of three different sequence strategies colliding in one block.\nEntry (AllGather): We cannot perform the QKV projection on a sharded sequence. We use AllGather(sp) to temporarily reconstruct the local sequence segment S/cp. Attention (CP): We use Context Parallelism here. Note that while the Query (Q) is local, the Keys (K) and Values (V) must be gathered over the cp ring (often overlapping computation with communication). Exit (ReduceScatter): The Output Projection (O Proj) is Row Parallel, meaning it results in partial sums sharded by D. We again use the ReduceScatter trick: we sum the partial results from TP and simultaneously reshard them back into the SP-sharded shape. Part III: MLP (Symmetry in Motion) The MLP block mirrors the Attention block’s communication pattern. This symmetry is vital for infrastructure design—it means your communication kernels can be reused across different layer types.\nThe flow remains consistent:\nGather Sequence (sp) Compute (TP): Gate/Up projections (Column Parallel) followed by Down projection (Row Parallel). Scatter Sequence (sp): Returning to the memory-efficient sharded shape. Part IV: Mixture of Experts (The Routing Complexity) This is the most complex diagram. We aren’t just sharding tensors; we are actively routing tokens to different devices.\nDispatch (AllToAll): This collective moves tokens from their sequence-sharded owners to their expert-sharded destinations. The shape changes from S based to Capacity (C) based. Inner Parallelism: Notice that inside the expert, we still apply Tensor Parallelism (F/tp). This is effectively “TP over EP.” Return: We have to ReduceScatter the inner TP results (resolving the partial sums from the expert’s Row Parallel output) before the final AllToAll returns the tokens to their original rank. Part V: Loss \u0026 The “Online Softmax” Calculating Cross Entropy when the vocab is sharded (V/vp) is non-trivial. You cannot simply take a softmax because the denominator requires a global sum.\nLogits: Computed via a Local Unembed (Vocab Parallel), resulting in logits sharded by V. Online Softmax: We perform two distinct collectives: AllReduce(max): To find the global max for numerical stability. AllReduce(sum): To calculate the denominator. Target Masking: Since the targets are on one rank but the logits are sharded, we mask the non-local logits and use AllReduce(sum) to broadcast the correct target logit to the group. Disclaimer \u0026 Context A Note on FSDP: You might notice FSDP (Fully Sharded Data Parallel) is intentionally left out of this specific accounting. This is because FSDP is fundamentally a memory optimization (sharding weights and optimizer states at rest) rather than a computation parallelism. FSDP gathers the full weights just-in-time for the forward/backward pass, meaning it does not affect the local tensor shapes that participate in the actual compute operations. I might cover FSDP and its interaction with these parallelisms in a follow-up post. A Note on “Best Practices”: These diagrams represent a snapshot of a common, representative setup found in modern frameworks like Megatron-Core or NVIDIA NeMo. It is not the only way to do it. The goal here is not to prescribe the “perfect” architecture, but to provide a visual language for composing these primitives. The challenge in infrastructure is making these parallelisms easy to understand, debug, and compose without sacrificing the performant timing of those critical collectives.\n","wordCount":"1136","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2026-01-19T22:12:33-08:00","dateModified":"2026-01-19T22:12:33-08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/parallelism-in-transformer/"},"publisher":{"@type":"Organization","name":"Ailing's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Parallelism in Transformer
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentColor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2026-01-19 22:12:33 -0800 PST'>January 19, 2026</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1136 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/parallelism-in-transformer.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=visualizing-the-distributed-transformer-an><strong>Visualizing the Distributed Transformer: An &ldquo;Accounting&rdquo; View of 4D Parallelism</strong><a hidden class=anchor aria-hidden=true href=#visualizing-the-distributed-transformer-an>#</a></h1><h2 id=motivation-simplicity-buried-in-abstractions><strong>Motivation: Simplicity buried in Abstractions</strong><a hidden class=anchor aria-hidden=true href=#motivation-simplicity-buried-in-abstractions>#</a></h2><p>I&rsquo;ve always loved the <a href=https://jax-ml.github.io/scaling-book/transformers/>&ldquo;Transformer Accounting&rdquo;</a> diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.</p><p>But as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the &lsquo;alphabet soup&rsquo; of 4D parallelism (DP, TP, SP, CP, EP).</p><p>Here is the irony: The core ideas behind these parallelisms are actually <strong>fundamentally easy</strong>. Conceptually, we are just slicing tensors. But in practice, this simplicity gets buried under layers of dense abstractions and &ldquo;spaghetti&rdquo; implementations in production codebases.</p><p>I created the diagrams below to cut through that noise. I want to restore that intuition by visually mapping the dance between <strong>Compute</strong> and <strong>Collectives</strong>. The goal is to show exactly <em>how</em> the compute is sharded and <em>when</em> the synchronization happens—stripping away the implementation complexity to reveal the simple logic underneath.</p><h2 id=how-to-read-these-diagrams><strong>How to Read These Diagrams</strong><a hidden class=anchor aria-hidden=true href=#how-to-read-these-diagrams>#</a></h2><p>To interpret the figures below, think of them as a map of what is happening on <strong>a single GPU</strong>.</p><h3 id=1-the-legends><strong>1. The Legends</strong><a hidden class=anchor aria-hidden=true href=#1-the-legends>#</a></h3><p>To navigate the maps, you need to know the symbols.</p><p><strong>Tensor Dimensions (The Global Object)</strong></p><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Definition</th><th style=text-align:left>Note</th></tr></thead><tbody><tr><td style=text-align:left><strong>B</strong></td><td style=text-align:left>Batch Size</td><td style=text-align:left>Global batch size across all devices.</td></tr><tr><td style=text-align:left><strong>S</strong></td><td style=text-align:left>Sequence Length</td><td style=text-align:left>Full sequence length (e.g., 4096, 128k).</td></tr><tr><td style=text-align:left><strong>D</strong></td><td style=text-align:left>Model Hidden Dimension</td><td style=text-align:left>The width of the residual stream.</td></tr><tr><td style=text-align:left><strong>V</strong></td><td style=text-align:left>Vocabulary Size</td><td style=text-align:left>Total size of the tokenizer vocabulary.</td></tr><tr><td style=text-align:left><strong>F</strong></td><td style=text-align:left>Feed-Forward Dimension</td><td style=text-align:left>Expansion dimension in MLP (usually 4*D).</td></tr><tr><td style=text-align:left><strong>E</strong></td><td style=text-align:left>Number of Experts</td><td style=text-align:left>Total experts in MoE layer.</td></tr><tr><td style=text-align:left><strong>C</strong></td><td style=text-align:left>Capacity</td><td style=text-align:left>Max tokens per expert (MoE specific).</td></tr></tbody></table><p><strong>The Mesh (The Slicing Strategy)</strong></p><p>These symbols represent the <strong>size</strong> of the process group used to shard a specific dimension.</p><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Definition</th><th style=text-align:left>Shards What?</th></tr></thead><tbody><tr><td style=text-align:left><strong>tp</strong></td><td style=text-align:left>Tensor Parallel</td><td style=text-align:left>Weights ($D$ or $F$) in Linear Layers</td></tr><tr><td style=text-align:left><strong>sp</strong></td><td style=text-align:left>Sequence Parallel</td><td style=text-align:left>Activations ($S$) in Element-wise Ops</td></tr><tr><td style=text-align:left><strong>cp</strong></td><td style=text-align:left>Context Parallel</td><td style=text-align:left>Sequence ($S$) in Attention (QKV)</td></tr><tr><td style=text-align:left><strong>ep</strong></td><td style=text-align:left>Expert Parallel</td><td style=text-align:left>Experts ($E$) in MoE Layers</td></tr><tr><td style=text-align:left><strong>vp</strong></td><td style=text-align:left>Vocab Parallel</td><td style=text-align:left>Vocabulary ($V$) in Embeddings/Loss</td></tr><tr><td style=text-align:left><strong>dp</strong></td><td style=text-align:left>Data Parallel</td><td style=text-align:left>Batch ($B$)</td></tr></tbody></table><h3 id=2-how-to-read-the-maps-visual-language><strong>2. How to Read the Maps (Visual Language)</strong><a hidden class=anchor aria-hidden=true href=#2-how-to-read-the-maps-visual-language>#</a></h3><p>To interpret the figures below, imagine you are sitting inside <strong>one single GPU</strong>.</p><ul><li><strong>Grey Boxes (Your Slice):</strong> These represent the data currently sitting in your GPU&rsquo;s memory.</li><li><strong>The Notation X/y:</strong> This tells you how the global tensor is cut. It means &ldquo;Take global dimension X and slice it into y parts.&rdquo;</li></ul><p><strong>Example: Reading the &ldquo;Local Shape&rdquo;</strong></p><p>You will frequently see this specific shape entering and exiting the major blocks:</p><p>[B/dp, S/(cp*sp), D]</p><p>Read this literally from left to right:</p><ol><li><strong>B/dp</strong>: &ldquo;I only hold a fraction of the Batch.&rdquo; (sliced by Data Parallelism)</li><li><strong>S/(cp*sp)</strong>: &ldquo;I only hold a tiny fragment of the Sequence.&rdquo; (sliced by both Context and Sequence Parallelism)</li><li><strong>D</strong>: &ldquo;I hold the <strong>full</strong> Hidden Model vector.&rdquo;</li></ol><p>Because you hold the full model vector D, you can perform element-wise operations (like LayerNorm or Residual Adds) locally without talking to anyone else.</p><h2 id=the-visual-walkthrough><strong>The Visual Walkthrough</strong><a hidden class=anchor aria-hidden=true href=#the-visual-walkthrough>#</a></h2><h3 id=part-i-embeddings--the><strong>Part I: Embeddings & The &ldquo;ReduceScatter&rdquo; Trick</strong><a hidden class=anchor aria-hidden=true href=#part-i-embeddings--the>#</a></h3><p>This layer sets the stage for the rest of the model.</p><ol><li><strong>Input:</strong> We start with tokens sharded by S/(cp*sp).</li><li><strong>Lookup:</strong> The embedding weights are sharded across the Vocab dimension (V/vp). This means a local lookup produces partial vectors (many are zeros).</li><li><strong>The Optimization:</strong> The naive approach would be to AllReduce (to sum the partial embeddings) and then Scatter (to shard them for Sequence Parallelism). Instead, we use a <strong>ReduceScatter</strong>. We sum the partial embeddings from the Vocab Parallel lookup and <em>immediately</em> scatter them into the Sequence Parallel dimension. This cuts communication overhead significantly right at the start.</li></ol><h3 id=part-ii-attention-tp--sp--cp><strong>Part II: Attention (TP + SP + CP)</strong><a hidden class=anchor aria-hidden=true href=#part-ii-attention-tp--sp--cp>#</a></h3><p>Here we see the complex interplay of three different sequence strategies colliding in one block.</p><ol><li><strong>Entry (AllGather):</strong> We cannot perform the QKV projection on a sharded sequence. We use AllGather(sp) to temporarily reconstruct the local sequence segment S/cp.</li><li><strong>Attention (CP):</strong> We use Context Parallelism here. Note that while the Query (Q) is local, the Keys (K) and Values (V) must be gathered over the cp ring (often overlapping computation with communication).</li><li><strong>Exit (ReduceScatter):</strong> The Output Projection (O Proj) is Row Parallel, meaning it results in partial sums sharded by D. We again use the ReduceScatter trick: we sum the partial results from TP and simultaneously reshard them back into the SP-sharded shape.</li></ol><h3 id=part-iii-mlp-symmetry-in-motion><strong>Part III: MLP (Symmetry in Motion)</strong><a hidden class=anchor aria-hidden=true href=#part-iii-mlp-symmetry-in-motion>#</a></h3><p>The MLP block mirrors the Attention block&rsquo;s communication pattern. This symmetry is vital for infrastructure design—it means your communication kernels can be reused across different layer types.</p><p>The flow remains consistent:</p><ol><li><strong>Gather Sequence (sp)</strong></li><li><strong>Compute (TP)</strong>: Gate/Up projections (Column Parallel) followed by Down projection (Row Parallel).</li><li><strong>Scatter Sequence (sp)</strong>: Returning to the memory-efficient sharded shape.</li></ol><h3 id=part-iv-mixture-of-experts-the-routing-complexity><strong>Part IV: Mixture of Experts (The Routing Complexity)</strong><a hidden class=anchor aria-hidden=true href=#part-iv-mixture-of-experts-the-routing-complexity>#</a></h3><p>This is the most complex diagram. We aren&rsquo;t just sharding tensors; we are actively routing tokens to different devices.</p><ol><li><strong>Dispatch (AllToAll):</strong> This collective moves tokens from their sequence-sharded owners to their expert-sharded destinations. The shape changes from S based to Capacity (C) based.</li><li><strong>Inner Parallelism:</strong> Notice that <em>inside</em> the expert, we still apply Tensor Parallelism (F/tp). This is effectively &ldquo;TP over EP.&rdquo;</li><li><strong>Return:</strong> We have to ReduceScatter the inner TP results (resolving the partial sums from the expert&rsquo;s Row Parallel output) <em>before</em> the final AllToAll returns the tokens to their original rank.</li></ol><h3 id=part-v-loss--the><strong>Part V: Loss & The &ldquo;Online Softmax&rdquo;</strong><a hidden class=anchor aria-hidden=true href=#part-v-loss--the>#</a></h3><p>Calculating Cross Entropy when the vocab is sharded (V/vp) is non-trivial. You cannot simply take a softmax because the denominator requires a global sum.</p><ol><li><strong>Logits:</strong> Computed via a Local Unembed (Vocab Parallel), resulting in logits sharded by V.</li><li><strong>Online Softmax:</strong> We perform two distinct collectives:<ul><li>AllReduce(max): To find the global max for numerical stability.</li><li>AllReduce(sum): To calculate the denominator.</li></ul></li><li><strong>Target Masking:</strong> Since the targets are on one rank but the logits are sharded, we mask the non-local logits and use AllReduce(sum) to broadcast the correct target logit to the group.</li></ol><h2 id=disclaimer--context><strong>Disclaimer & Context</strong><a hidden class=anchor aria-hidden=true href=#disclaimer--context>#</a></h2><p>A Note on FSDP:
You might notice FSDP (Fully Sharded Data Parallel) is intentionally left out of this specific accounting. This is because FSDP is fundamentally a memory optimization (sharding weights and optimizer states at rest) rather than a computation parallelism. FSDP gathers the full weights just-in-time for the forward/backward pass, meaning it does not affect the local tensor shapes that participate in the actual compute operations. I might cover FSDP and its interaction with these parallelisms in a follow-up post.
A Note on &ldquo;Best Practices&rdquo;:
These diagrams represent a snapshot of a common, representative setup found in modern frameworks like Megatron-Core or NVIDIA NeMo. It is not the only way to do it. The goal here is not to prescribe the &ldquo;perfect&rdquo; architecture, but to provide a visual language for composing these primitives. The challenge in infrastructure is making these parallelisms easy to understand, debug, and compose without sacrificing the performant timing of those critical collectives.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/pipeline-parallelism-demystified/><span class=title>Next »</span><br><span>Pipeline Parallelism Demystified</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on x" href="https://x.com/intent/tweet/?text=Parallelism%20in%20Transformer&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f&amp;title=Parallelism%20in%20Transformer&amp;summary=Parallelism%20in%20Transformer&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f&title=Parallelism%20in%20Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on whatsapp" href="https://api.whatsapp.com/send?text=Parallelism%20in%20Transformer%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on telegram" href="https://telegram.me/share/url?text=Parallelism%20in%20Transformer&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in Transformer on ycombinator" href="https://news.ycombinator.com/submitlink?t=Parallelism%20in%20Transformer&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fparallelism-in-transformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Ailing's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>