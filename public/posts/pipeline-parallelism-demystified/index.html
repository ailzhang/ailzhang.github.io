<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pipeline Parallelism Demystified | My New Hugo Site</title><meta name=keywords content><meta name=description content="In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.
Pipeline parallelism? It barely got a page.
At the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/pipeline-parallelism-demystified/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/pipeline-parallelism-demystified/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/pipeline-parallelism-demystified/"><meta property="og:site_name" content="My New Hugo Site"><meta property="og:title" content="Pipeline Parallelism Demystified"><meta property="og:description" content="In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.
Pipeline parallelism? It barely got a page.
At the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-03T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-03T00:00:00+00:00"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Pipeline Parallelism Demystified"><meta name=twitter:description content="In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.
Pipeline parallelism? It barely got a page.
At the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Pipeline Parallelism Demystified","item":"http://localhost:1313/posts/pipeline-parallelism-demystified/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pipeline Parallelism Demystified","name":"Pipeline Parallelism Demystified","description":"In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.\nPipeline parallelism? It barely got a page.\nAt the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall.\n","keywords":[],"articleBody":"In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.\nPipeline parallelism? It barely got a page.\nAt the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall.\nThose neat diagrams in papers always make it look so straightforward‚Ä¶ until you dive in. Then it turns into tangled schedules, weird idle bubbles, and code that‚Äôs tightly coupled with other components.\nSo here‚Äôs the chapter that probably should‚Äôve been in the book: an intuition-first, no-formula guide to pipeline parallelism‚Äîwhat it is, why it‚Äôs tricky, and how we ended up with so many different flavors of it.\nBonus point: a toy PP implementation for all schedules While writing this post, I realized it was surprisingly hard to compare pipeline schedules across papers‚Äîeveryone presents their plots in slightly different setups, which makes apples-to-apples comparisons tricky. As a result I went down the rabbit hole and built a toy simulator called minPP.\nIt‚Äôs a lightweight, CPU-only tool that lets you:\nDefine arbitrary partitioning + assignment + execution combos Simulate step-by-step microbatch flows in timeline Visualize schedules in a consistent format It‚Äôs just under 1,000 lines of code, definitely not production grade, still has some rough edges that I haven‚Äôt got time to fix. But it helped me a lot in understanding and comparing schedules in a clean, structured way. And more importantly: every plot in this post is generated using it.\nHonestly, I probably spent more time writing the simulator than I would‚Äôve spent drawing the schedules by hand. But hey, now it‚Äôs reusable üòÖ.\nKey questions in pipeline parallelism Large language models (LLMs) are basically giant directed acyclic graphs (DAGs) of layers‚Äîa.k.a. compute graphs in most ML frameworks. Data flows through these graphs step by step, like items on a conveyor belt.\nWhen the model gets too big to fit on a single GPU, but you have multiple GPUs. A natural idea is:\nBreak the model into chunks (a few layers each), assign each chunk to a GPU, and stream the inputs through them.\nCongratulations‚Äîyou‚Äôve pipelined your model.But actually making that work in practice boils down to answering three key questions in this section.\nTo keep things simple, we‚Äôll use the following setup throughout this post:\n8 layers in the model 4 identical GPUs This makes it easier to compare different partitioning, assignment, and execution strategies side by side.\nHow should we partiton the model? A straightforward partition might be just partition the model to 4 parts:\n[0, 1]: from layer 0 to 1 [2, 3]: from layer 2 to 3 [4, 5]: from layer 4 to 5 [6, 7]: from layer 5 to 7 Each group of consecutive layers like above is called a pipeline stage. It‚Äôs a self-contained unit that handles both forward and backward passes of layers. We use a bracket [x, y] to represent a stage‚Äôs start \u0026 end layer(inclusive).\nNote that we currently have 2 layers per stage, but you could do 1 layer per stage too, resulting in 8 stages in total. We will see in later sections on how schedules leverage this finer granularity partitions.\nHow do we assign stages to GPUs? Once you‚Äôbe defined your stages, it‚Äôs time to assign them to GPUs. The simplest approach is one stage per GPU and assign them sequentially.\nBut what if you have more stages than GPUs, the assignment ordering can be quite flexible and potentially more interesting. Looping and vshape assignment are the mostly commonly used ones. If we do 8 stages in total(1 layer per stage) they look like below:\nLooping assignment Vshape assignment GPU 0 [0], [4] [0], [7] GPU 1 [1], [5] [1], [6] GPU 2 [2], [6] [2], [5] GPU 3 [3], [7] [3], [4] Note that there‚Äôs probably no single ‚Äúcorrect‚Äù assignment, the way you assign stages to devive can impact load balance, communication patterns and even how well you can overlap communication.\nHow do we priortize different microbatches? To avoid idle GPUs (aka pipeline bubbles), we split the input batch into microbatches and schedule them across pipeline stages. Once stages are assigned to devices, we know exactly how a microbatch flows through across the devices. But given we have multiple microbatch in flight and each of them might have forward and backward pass to run, and usually the execution takes the GPU exclusively. Here is the main challenge: how do we priortize local execution on each GPU?\nSay forward(i) is the forward pass of microbatch i. On a given GPU:\nforward(i) must run before backward(i) forward(i) must run before forward(i+1) But there‚Äôs still a lot of room for ordering, for example, when both backward(i) and forward(i+1) are ready, which do you run first?\nIn general you have two choices here:\nOption Description Priority Depth first Run backward(i) first Push one microbatch through all stages Breadth first Run forward(i+1) first Process all microbatches in one stage This choice defines your execution schedule.\nSo far, we‚Äôve assumed each stage‚Äôs forward or backward execution takes the whole GPU exclusively. But some schedules (like DualPipe) can run 1 foward and 1 backward concurrently, and that changes the picture entirely. More on that soon.\nReady for some real schedules? Now that we‚Äôve nailed the three knobs: partitioning, assignment, and execution‚Äîlet‚Äôs look at some real schedules.\nI‚Äôve always liked those timeline figures in papers. They‚Äôre well thought out and show how microbatches flow through devices. But they can be hard to follow when the stage-to-device mapping is hidden. That mapping is usually straightforward, but once you start doing more advanced partitioning or stage allocation, it introduces an extra layer of indirection and makes understanding hard.\nTo keep things simple, we‚Äôll collapse the layer ‚Üí stage ‚Üí device mapping into a direct layer ‚Üí device mapping. We will continue using the stage notation above. If multiple stages are assigned to one device, they are separated by a comma.\nAs every pipeline parallelism tutorial seems to do, let‚Äôs start with Gpipe and 1F1B, and see if cleanly separating these three questions (partitioning, assignment, execution) helps us better understand how it all works.\nGpipe and 1F1B Gpipe 1F1B num_stage_per_GPU 1 1 num_layers_per_stage 2 2 GPU 0 [0, 1] [0, 1] GPU 1 [2, 3] [2, 3] GPU 2 [4, 5] [4, 5] GPU 3 [6, 7] [6, 7] Execution order BFS DFS You can see from the table above that Gpipe and 1F1B only differs in how they priortize execution locally.\nGpipe Breadth-first scheduling (BFS) is great for small batch size training. It‚Äôs simple to implement and easy to reason about.\nBut there‚Äôs a catch: since BFS keeps all microbatches in flight before any backward pass starts, the memory used for storing intermediate activations builds up. All those tensors need to stay alive until the corresponding backward happens.\nSo while BFS based Gpipe is efficient in terms of throughput, it puts a lot of pressure on peak memory consumption.\nYou can apply activation checkpointing to reduce memory usage. But that comes at the cost of additional recomputation and slower runtime.\n1F1B On the other hand, DFS is much more memory friendly. Switching Gpipe to use DFS is what we called original 1F1B schedule. Each rank runs 1 Forward + 1 Backward in alternating fashion. DFS allows each GPU to prioritize backward passes over forward passes when both are ready, which reduces the number of activations that need to be held in memory. As noted in the figure.\nIt‚Äôs worth noting: while 1F1B reduces peak memory usage, it doesn‚Äôt fix the pipeline bubble problem. You still end up with idle time in the early and late stages of the pipeline.\nAlgorithm based optimizations Before we get to more complex schedules, there‚Äôre also optimizations coming from some prior knowledge in model architecture that we can leverage. These optimizations can be enabled on top of the 1F1B schedule easily.\nSchedule Idea Zero bubble 1F1B priortize dgrad to execute first since it‚Äôs on critical path Eager 1F1B takes into data transfer time into accout DualPipe takes into data transfer time into accout ZB1F1B ZeroBubble 1F1B takes things a step further by looking inside the backward pass.\nEach backward consists of two major compute ops:\ndgrad (gradient w.r.t. inputs): needed immediately to unblock the next stage wgrad (gradient w.r.t. weights): used only locally The key insight here is only dgrad is on the critical path. So we can deprioritize wgrad and use it to fill in the idle gaps between critical ops.\nThis trick can be applied to most schedules‚Äîbut it‚Äôs especially helpful for DFS-style ones, where backward is already prioritized.\nEager 1F1B In all the schedules above, we‚Äôve assumed that inter-device communication is instantaneous. As soon as stage i finishes its output, stage i+1 can start right away.\nThis is a useful simplification for understanding schedules, but in practice, communication time isn‚Äôt negligible.\nEager 1F1B addresses this by kicking off computations as early as possible to create opportunities for overlapping communication and computation.\nHelps overlap forward compute and communication Does not improve backward overlap Ô∏èIncreases memory usage due to more in-flight microbatches So, like many things in pipeline parallelism, it‚Äôs a trade-off: you gain throughput by overlapping forward stages, but at the cost of higher memory consumption.\nDualPipe: Bidirectional 1F1B DualPipe (extended from Chimera) takes 1F1B optimization in a different direction‚Äîliterally.\nInstead of running a single dataflow from the first stage to the last, it runs two 1F1B schedules simultaneously in opposite directions. It overlays them into one bidirectional pipeline.\nDualPipe num_layers_per_stage 2 num_stage_per_GPU 2 GPU 0 [0, 1], [6, 7] GPU 1 [2, 3], [4, 6] GPU 2 [4, 5], [3, 2] GPU 3 [6, 7], [0, 1] Execution order DFS Sounds a bit confusing? Let‚Äôs break it down.\nDualPipe is designed to make better use of low-bandwidth interconnects between devices. It does this by running a forward pass and a backward pass at the same time, but: The forward and backward passes are from different microbatches And from different pipeline directions (normal vs. reversed) This trick works well in large sparse MoE models, where the All2All communication and GEMM compute take roughly the same amount of time‚Äîallowing communication and computation to overlap cleanly.\nOf course, nothing comes for free.\nDualPipe doubles the memory footprint, since each device now stores 4 layers instead of 2. But in return, it achieves better compute-communication overlap and higher pipeline utilization. I didn‚Äôt really implement this schedule, as acknowledged in the DualPipe repo, this schedule can be greatly simplified to DualPipeV schedule we will show below.\nAdvanced schedules Interleaved virtual pipeline Vshape zero bubble DualPipeV BFS-looping num_layers_per_stage 1 1 1 1 num_stage_per_GPU 2 2 2 2 assignment looping vshape vshape looping GPU 0 [0], [4] [0], [7] [0],[7] [0],[4] GPU 1 [1], [5] [1], [6] [1],[6] [1],[5] GPU 2 [2], [6] [2], [5] [2],[5] [2],[6] GPU 3 [3], [7] [3], [4] [3],[4] [3],[7] Execution order DFS DFS DFS BFS Optimization consider data transfer time zero bubble EP None Interleaved virtual pipeline (Megatron) The MegatronLM paper introduced a clever way to reduce pipeline bubbles. By dividing the model into smaller stages and assigning multiple stages per device, you can shrink idle time. This schedule is called interleaved 1F1B.\nIn the example interleaved virtual pipeline above, we basically now have 1 layer per stage but 2 stages per device. To finish one microbatch forward, you‚Äôll need to loop over all devices twice(marked in the image). The bubble time is smaller, that comes at the cost of more communication as we have more pipeline stages, we also need to communicate more across devices.\nNote in interleaved 1F1B, when we have assign stages to devices, we assign stages by looping over devices. This creates a memory bottleneck on the first device and under utilization of memory on all other device. This imbalance in memory usage can be mitigated by the vshape schedule below.\nAlso note that interleaved is also takes in data transfer time into account and tries to overlap communication and compute as much as possible.\nVshape-ZB The lifespan of a stage is defined as the time between the start of its forward pass and the end of its backward pass.\nIn both Interleaved 1F1B and V-shape, the execution schedule is the same. The only difference lies in how stages are assigned to devices.\nSince peak memory usage is proportional to the stage‚Äôs lifespan, the default interleaved schedule (which assigns stages round-robin) can lead to memory hotspots, especially on the first device.\nThe V-shape assignment fixes this by intentionally collocating:\nStages with long lifespans Stages with short lifespans This creates a more balanced memory footprint across all devices, reducing bottlenecks without changing the overall schedule behavior.\nNote we picked vshape-zb schedule in the paper to show here which also employs zero bubble optimization abov\nDualPipeV As mentioned above, the 2x memory usage in DualPipe schedule is actually quite expensive. DualPipeV actually shows that the dual memory usuage is not necessary and it can be simplified by cutting in half. Then it can be viewed as a variant of vshape-zb schedule above by running 1 forward and 1 backward simultaneously.\nBFS-looping While DFS-based schedules (like 1F1B and its variants) dominate in practice, they come with a key limitation. They require a large number of microbatches to fully utilize the pipeline‚Äîwhich isn‚Äôt always feasible.\nTo address this, breath-first PP paper proposed an alternative:\nUse similar virtual pipeline stages as in interleaved but use BFS (instead of DFS) for execution Combine it with FSDP and activation checkpointing to reduce peak memory usage. This combo turns out to be helpful in small batch size settings, where DFS would otherwise stall or waste resources due to insufficient microbatches.\nCommon Questions Q: What if a single layer can‚Äôt fit on a GPU?\nA: Then PP isn‚Äôt enough. You‚Äôll need tensor parallelism (TP), where multiple GPUs jointly run one operation (e.g. matrix multiply).\nQ: Why is scheduling so hard?\nA: Because you‚Äôre solving a data dependency scheduling problem under communication and compute constraints. Minimizing idle time while respecting order is‚Ä¶ hard.\nQ: Why not automate it?\nA: Great idea! In fact, many teams are trying. But a proper scheduler needs to know model structure, compute cost, activation sizes, hardware bandwidths, and more. It‚Äôs a compiler problem‚Äîbut compilers for distributed training are still in early days.\nClosing Thoughts I don‚Äôt really expect this post to be as detailed as the papers so I‚Äôve intentionally excluded all the formulas here. But hopefully it gives you enough intuition to navigate them with more confidence.\nAt its core, pipeline parallelism is a simple idea. But the implementation is a surprisingly deep rabbit hole.\n","wordCount":"2470","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-08-03T00:00:00Z","dateModified":"2025-08-03T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/pipeline-parallelism-demystified/"},"publisher":{"@type":"Organization","name":"My New Hugo Site","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Pipeline Parallelism Demystified</h1><div class=post-meta><span title='2025-08-03 00:00:00 +0000 UTC'>August 3, 2025</span>&nbsp;¬∑&nbsp;12 min&nbsp;¬∑&nbsp;2470 words&nbsp;¬∑&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/pipeline-parallelism-demystified/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>In our book Efficient PyTorch, we gave a quick overview of the main sharding strategies used in large-scale distributed training: data parallelism, tensor parallelism, pipeline parallelism, and a few Transformer-specific ones like expert parallelism and context parallelism.</p><p>Pipeline parallelism? It barely got a page.</p><p>At the time, I thought it was too intuitive to need much detail. Then last week, I tried explaining all the existing schedules in a clean, logical way‚Äîand completely hit a wall.</p><p>Those neat diagrams in papers always make it look so straightforward‚Ä¶ until you dive in. Then it turns into tangled schedules, weird idle bubbles, and code that‚Äôs tightly coupled with other components.</p><p>So here‚Äôs the chapter that probably should‚Äôve been in the book: an intuition-first, no-formula guide to pipeline parallelism‚Äîwhat it is, why it‚Äôs tricky, and how we ended up with so many different flavors of it.</p><h3 id=bonus-point-a-toy-pp-implementation-for-all-schedules>Bonus point: a toy PP implementation for all schedules<a hidden class=anchor aria-hidden=true href=#bonus-point-a-toy-pp-implementation-for-all-schedules>#</a></h3><p>While writing this post, I realized it was surprisingly hard to compare pipeline schedules across papers‚Äîeveryone presents their plots in slightly different setups, which makes apples-to-apples comparisons tricky. As a result I went down the rabbit hole and built a toy simulator called <a href=https://github.com/ailzhang/minPP/tree/main>minPP</a>.</p><p>It‚Äôs a lightweight, CPU-only tool that lets you:</p><ul><li>Define arbitrary partitioning + assignment + execution combos</li><li>Simulate step-by-step microbatch flows in timeline</li><li>Visualize schedules in a consistent format</li></ul><p>It‚Äôs just under 1,000 lines of code, definitely not production grade, still has some rough edges that I haven&rsquo;t got time to fix. But it helped me a lot in understanding and comparing schedules in a clean, structured way. And more importantly: <strong>every plot in this post is generated using it</strong>.</p><p>Honestly, I probably spent more time writing the simulator than I would‚Äôve spent drawing the schedules by hand. But hey, now it‚Äôs reusable üòÖ.</p><h1 id=key-questions-in-pipeline-parallelism>Key questions in pipeline parallelism<a hidden class=anchor aria-hidden=true href=#key-questions-in-pipeline-parallelism>#</a></h1><p>Large language models (LLMs) are basically giant directed acyclic graphs (DAGs) of layers‚Äîa.k.a. compute graphs in most ML frameworks. Data flows through these graphs step by step, like items on a conveyor belt.</p><p>When the model gets too big to fit on a single GPU, but you have multiple GPUs. A natural idea is:</p><blockquote><p>Break the model into chunks (a few layers each), assign each chunk to a GPU, and stream the inputs through them.</p></blockquote><p>Congratulations‚Äîyou‚Äôve pipelined your model.But actually making that work in practice boils down to answering three key questions in this section.</p><p>To keep things simple, we‚Äôll use the following setup throughout this post:</p><ul><li>8 layers in the model</li><li>4 identical GPUs</li></ul><p>This makes it easier to compare different partitioning, assignment, and execution strategies side by side.</p><h2 id=how-should-we-partiton-the-model>How should we partiton the model?<a hidden class=anchor aria-hidden=true href=#how-should-we-partiton-the-model>#</a></h2><p>A straightforward partition might be just partition the model to 4 parts:</p><ul><li>[0, 1]: from layer 0 to 1</li><li>[2, 3]: from layer 2 to 3</li><li>[4, 5]: from layer 4 to 5</li><li>[6, 7]: from layer 5 to 7</li></ul><p>Each group of consecutive layers like above is called a <strong>pipeline stage</strong>. It&rsquo;s a self-contained unit that handles both forward and backward passes of layers. We use a bracket <code>[x, y]</code> to represent a stage&rsquo;s start & end layer(inclusive).</p><p>Note that we currently have 2 layers per stage, but you could do 1 layer per stage too, resulting in 8 stages in total. We will see in later sections on how schedules leverage this finer granularity partitions.</p><h2 id=how-do-we-assign-stages-to-gpus>How do we assign stages to GPUs?<a hidden class=anchor aria-hidden=true href=#how-do-we-assign-stages-to-gpus>#</a></h2><p>Once you&rsquo;be defined your stages, it&rsquo;s time to assign them to GPUs. The simplest approach is one stage per GPU and assign them sequentially.</p><p>But what if you have more stages than GPUs, the assignment ordering can be quite flexible and potentially more interesting. Looping and vshape assignment are the mostly commonly used ones. If we do 8 stages in total(1 layer per stage) they look like below:</p><table><thead><tr><th></th><th>Looping assignment</th><th>Vshape assignment</th></tr></thead><tbody><tr><td>GPU 0</td><td>[0], [4]</td><td>[0], [7]</td></tr><tr><td>GPU 1</td><td>[1], [5]</td><td>[1], [6]</td></tr><tr><td>GPU 2</td><td>[2], [6]</td><td>[2], [5]</td></tr><tr><td>GPU 3</td><td>[3], [7]</td><td>[3], [4]</td></tr></tbody></table><p>Note that there&rsquo;s probably no single &ldquo;correct&rdquo; assignment, the way you assign stages to devive can impact load balance, communication patterns and even how well you can overlap communication.</p><h2 id=how-do-we-priortize-different-microbatches>How do we priortize different microbatches?<a hidden class=anchor aria-hidden=true href=#how-do-we-priortize-different-microbatches>#</a></h2><p>To avoid idle GPUs (aka pipeline bubbles), we split the input batch into microbatches and schedule them across pipeline stages. Once stages are assigned to devices, we know exactly how a microbatch flows through <strong>across the devices</strong>. But given we have multiple microbatch in flight and each of them might have forward and backward pass to run, and usually the execution takes the GPU exclusively. Here is the main challenge: how do we priortize local execution on each GPU?</p><p>Say <code>forward(i)</code> is the forward pass of microbatch <code>i</code>. On a given GPU:</p><ul><li><code>forward(i)</code> must run before <code>backward(i)</code></li><li><code>forward(i)</code> must run before <code>forward(i+1)</code></li></ul><p>But there&rsquo;s still a lot of room for ordering, for example, when both <code>backward(i)</code> and <code>forward(i+1)</code> are ready, which do you run first?</p><p>In general you have two choices here:</p><table><thead><tr><th>Option</th><th>Description</th><th>Priority</th></tr></thead><tbody><tr><td>Depth first</td><td>Run <code>backward(i)</code> first</td><td>Push one microbatch through all stages</td></tr><tr><td>Breadth first</td><td>Run <code>forward(i+1)</code> first</td><td>Process all microbatches in one stage</td></tr></tbody></table><p>This choice defines your execution schedule.</p><blockquote><p>So far, we‚Äôve assumed each stage&rsquo;s forward or backward execution takes the whole GPU exclusively. But some schedules (like DualPipe) can run 1 foward and 1 backward concurrently, and that changes the picture entirely. More on that soon.</p></blockquote><h1 id=ready-for-some-real-schedules>Ready for some real schedules?<a hidden class=anchor aria-hidden=true href=#ready-for-some-real-schedules>#</a></h1><p>Now that we‚Äôve nailed the three knobs: partitioning, assignment, and execution‚Äîlet‚Äôs look at some real schedules.</p><p>I‚Äôve always liked those timeline figures in papers. They‚Äôre well thought out and show how microbatches flow through devices. But they can be hard to follow when the stage-to-device mapping is hidden. That mapping is usually straightforward, but once you start doing more advanced partitioning or stage allocation, it introduces an extra layer of indirection and makes understanding hard.</p><p>To keep things simple, we‚Äôll collapse the layer ‚Üí stage ‚Üí device mapping into a direct layer ‚Üí device mapping. We will continue using the stage notation above. If multiple stages are assigned to one device, they are separated by a comma.</p><p>As every pipeline parallelism tutorial seems to do, let‚Äôs start with Gpipe and 1F1B, and see if cleanly separating these three questions (partitioning, assignment, execution) helps us better understand how it all works.</p><h2 id=gpipe-and-1f1b>Gpipe and 1F1B<a hidden class=anchor aria-hidden=true href=#gpipe-and-1f1b>#</a></h2><table><thead><tr><th></th><th>Gpipe</th><th>1F1B</th></tr></thead><tbody><tr><td>num_stage_per_GPU</td><td>1</td><td>1</td></tr><tr><td>num_layers_per_stage</td><td>2</td><td>2</td></tr><tr><td>GPU 0</td><td>[0, 1]</td><td>[0, 1]</td></tr><tr><td>GPU 1</td><td>[2, 3]</td><td>[2, 3]</td></tr><tr><td>GPU 2</td><td>[4, 5]</td><td>[4, 5]</td></tr><tr><td>GPU 3</td><td>[6, 7]</td><td>[6, 7]</td></tr><tr><td>Execution order</td><td>BFS</td><td>DFS</td></tr></tbody></table><p>You can see from the table above that Gpipe and 1F1B only differs in how they priortize execution locally.</p><h3 id=gpipe>Gpipe<a hidden class=anchor aria-hidden=true href=#gpipe>#</a></h3><p>Breadth-first scheduling (BFS) is great for small batch size training. It‚Äôs simple to implement and easy to reason about.</p><p>But there‚Äôs a catch: since BFS keeps all microbatches in flight before any backward pass starts, the memory used for storing intermediate activations builds up. All those tensors need to stay alive until the corresponding backward happens.</p><p>So while <a href=https://arxiv.org/abs/1811.06965>BFS based Gpipe</a> is efficient in terms of throughput, it puts a lot of pressure on peak memory consumption.</p><p>You can apply activation checkpointing to reduce memory usage. But that comes at the cost of additional recomputation and slower runtime.</p><p><img alt="Gpipe schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/gpipe-noted.png></p><h3 id=1f1b>1F1B<a hidden class=anchor aria-hidden=true href=#1f1b>#</a></h3><p>On the other hand, DFS is much more memory friendly. Switching Gpipe to use DFS is what we called original <a href=https://arxiv.org/pdf/1806.03377>1F1B schedule</a>. Each rank runs 1 Forward + 1 Backward in alternating fashion. DFS allows each GPU to prioritize backward passes over forward passes when both are ready, which reduces the number of activations that need to be held in memory. As noted in the figure.</p><p><img alt="1F1B schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/1f1b-noted.png></p><p>It‚Äôs worth noting: while 1F1B reduces peak memory usage, it doesn‚Äôt fix the pipeline bubble problem. You still end up with idle time in the early and late stages of the pipeline.</p><h2 id=algorithm-based-optimizations>Algorithm based optimizations<a hidden class=anchor aria-hidden=true href=#algorithm-based-optimizations>#</a></h2><p>Before we get to more complex schedules, there&rsquo;re also optimizations coming from some prior knowledge in model architecture that we can leverage. These optimizations can be enabled on top of the 1F1B schedule easily.</p><table><thead><tr><th>Schedule</th><th>Idea</th></tr></thead><tbody><tr><td>Zero bubble 1F1B</td><td>priortize dgrad to execute first since it&rsquo;s on critical path</td></tr><tr><td>Eager 1F1B</td><td>takes into data transfer time into accout</td></tr><tr><td>DualPipe</td><td>takes into data transfer time into accout</td></tr></tbody></table><h3 id=zb1f1b>ZB1F1B<a hidden class=anchor aria-hidden=true href=#zb1f1b>#</a></h3><p><a href=https://arxiv.org/abs/2401.10241>ZeroBubble 1F1B</a> takes things a step further by looking inside the backward pass.</p><p>Each backward consists of two major compute ops:</p><ul><li>dgrad (gradient w.r.t. inputs): needed immediately to unblock the next stage</li><li>wgrad (gradient w.r.t. weights): used only locally</li></ul><p>The key insight here is <strong>only dgrad is on the critical path</strong>.
So we can deprioritize wgrad and use it to fill in the idle gaps between critical ops.</p><p><img alt="Zero bubble 1F1B schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/zb1f1b-noted.png></p><p>This trick can be applied to most schedules‚Äîbut it‚Äôs especially helpful for DFS-style ones, where backward is already prioritized.</p><h3 id=eager-1f1b>Eager 1F1B<a hidden class=anchor aria-hidden=true href=#eager-1f1b>#</a></h3><p>In all the schedules above, we‚Äôve assumed that inter-device communication is instantaneous. As soon as stage <code>i</code> finishes its output, stage <code>i+1</code> can start right away.</p><p>This is a useful simplification for understanding schedules, but in practice, communication time isn‚Äôt negligible.</p><p><a href=https://arxiv.org/pdf/2211.05322>Eager 1F1B</a> addresses this by kicking off computations as early as possible to create opportunities for overlapping communication and computation.</p><ul><li>Helps overlap forward compute and communication</li><li>Does not improve backward overlap</li><li>Ô∏èIncreases memory usage due to more in-flight microbatches</li></ul><p>So, like many things in pipeline parallelism, it‚Äôs a trade-off: you gain throughput by overlapping forward stages, but at the cost of higher memory consumption.</p><p><img alt="Eager 1F1B schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/eager1f1b-noted.png></p><h3 id=dualpipe-bidirectional-1f1b>DualPipe: Bidirectional 1F1B<a hidden class=anchor aria-hidden=true href=#dualpipe-bidirectional-1f1b>#</a></h3><p><a href=https://arxiv.org/html/2412.19437v1>DualPipe</a> (extended from <a href=https://arxiv.org/abs/2107.06925>Chimera</a>) takes 1F1B optimization in a different direction‚Äîliterally.</p><p>Instead of running a single dataflow from the first stage to the last, it runs two 1F1B schedules simultaneously in opposite directions.
It overlays them into one bidirectional pipeline.</p><table><thead><tr><th></th><th>DualPipe</th></tr></thead><tbody><tr><td>num_layers_per_stage</td><td>2</td></tr><tr><td>num_stage_per_GPU</td><td>2</td></tr><tr><td>GPU 0</td><td>[0, 1], [6, 7]</td></tr><tr><td>GPU 1</td><td>[2, 3], [4, 6]</td></tr><tr><td>GPU 2</td><td>[4, 5], [3, 2]</td></tr><tr><td>GPU 3</td><td>[6, 7], [0, 1]</td></tr><tr><td>Execution order</td><td>DFS</td></tr></tbody></table><p>Sounds a bit confusing? Let‚Äôs break it down.</p><ul><li>DualPipe is designed to make better use of low-bandwidth interconnects between devices.</li><li>It does this by <strong>running a forward pass and a backward pass at the same time</strong>, but:<ul><li>The forward and backward passes are from different microbatches</li><li>And from different pipeline directions (normal vs. reversed)</li></ul></li></ul><p>This trick works well in large sparse MoE models, where the All2All communication and GEMM compute take roughly the same amount of time‚Äîallowing communication and computation to overlap cleanly.</p><p>Of course, nothing comes for free.</p><ul><li>DualPipe doubles the memory footprint, since each device now stores 4 layers instead of 2.</li><li>But in return, it achieves better compute-communication overlap and higher pipeline utilization.</li></ul><p>I didn&rsquo;t really implement this schedule, as acknowledged in the <a href=https://github.com/deepseek-ai/DualPipe#dualpipev>DualPipe repo</a>, this schedule can be greatly simplified to DualPipeV schedule we will show below.</p><h2 id=advanced-schedules>Advanced schedules<a hidden class=anchor aria-hidden=true href=#advanced-schedules>#</a></h2><table><thead><tr><th></th><th>Interleaved virtual pipeline</th><th>Vshape zero bubble</th><th>DualPipeV</th><th>BFS-looping</th></tr></thead><tbody><tr><td>num_layers_per_stage</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>num_stage_per_GPU</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>assignment</td><td>looping</td><td>vshape</td><td>vshape</td><td>looping</td></tr><tr><td>GPU 0</td><td>[0], [4]</td><td>[0], [7]</td><td>[0],[7]</td><td>[0],[4]</td></tr><tr><td>GPU 1</td><td>[1], [5]</td><td>[1], [6]</td><td>[1],[6]</td><td>[1],[5]</td></tr><tr><td>GPU 2</td><td>[2], [6]</td><td>[2], [5]</td><td>[2],[5]</td><td>[2],[6]</td></tr><tr><td>GPU 3</td><td>[3], [7]</td><td>[3], [4]</td><td>[3],[4]</td><td>[3],[7]</td></tr><tr><td>Execution order</td><td>DFS</td><td>DFS</td><td>DFS</td><td>BFS</td></tr><tr><td>Optimization</td><td>consider data transfer time</td><td>zero bubble</td><td>EP</td><td>None</td></tr></tbody></table><h3 id=interleaved-virtual-pipeline-megatron>Interleaved virtual pipeline (Megatron)<a hidden class=anchor aria-hidden=true href=#interleaved-virtual-pipeline-megatron>#</a></h3><p>The <a href=https://arxiv.org/pdf/2104.04473>MegatronLM paper</a> introduced a clever way to reduce pipeline bubbles. By dividing the model into smaller stages and assigning multiple stages per device, you can shrink idle time. This schedule is called interleaved 1F1B.</p><p>In the example interleaved virtual pipeline above, we basically now have 1 layer per stage but 2 stages per device. To finish one microbatch forward, you&rsquo;ll need to loop over all devices twice(marked in the image). The bubble time is smaller, that comes at the cost of more communication as we have more pipeline stages, we also need to communicate more across devices.</p><p>Note in interleaved 1F1B, when we have assign stages to devices, we assign stages by looping over devices. This creates a memory bottleneck on the first device and under utilization of memory on all other device. This imbalance in memory usage can be mitigated by the vshape schedule below.</p><p>Also note that interleaved is also takes in data transfer time into account and tries to overlap communication and compute as much as possible.</p><p><img alt="Interleaved virtual pipeline schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/interleaved-noted.png></p><h3 id=vshape-zb>Vshape-ZB<a hidden class=anchor aria-hidden=true href=#vshape-zb>#</a></h3><p>The lifespan of a stage is defined as the time between the start of its forward pass and the end of its backward pass.</p><p>In both Interleaved 1F1B and V-shape, the execution schedule is the same. The only difference lies in how stages are assigned to devices.</p><p>Since peak memory usage is proportional to the stage‚Äôs lifespan, the default interleaved schedule (which assigns stages round-robin) can lead to memory hotspots, especially on the first device.</p><p>The <a href=https://arxiv.org/pdf/2405.15362>V-shape</a> assignment fixes this by intentionally collocating:</p><ul><li>Stages with long lifespans</li><li>Stages with short lifespans</li></ul><p>This creates a more balanced memory footprint across all devices, reducing bottlenecks without changing the overall schedule behavior.</p><p><img alt="Vshape zb schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/vshape_zb-noted.png></p><p>Note we picked vshape-zb schedule in the paper to show here which also employs zero bubble optimization abov</p><h3 id=dualpipev>DualPipeV<a hidden class=anchor aria-hidden=true href=#dualpipev>#</a></h3><p>As mentioned above, the 2x memory usage in DualPipe schedule is actually quite expensive. <a href=https://hackmd.io/@ufotalent/r1lVXsa9Jg>DualPipeV</a> actually shows that the dual memory usuage is not necessary and it can be simplified by cutting in half. Then it can be viewed as a variant of vshape-zb schedule above by running 1 forward and 1 backward simultaneously.</p><p><img alt="Vshape zb schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/dualpipev-noted.png></p><h3 id=bfs-looping>BFS-looping<a hidden class=anchor aria-hidden=true href=#bfs-looping>#</a></h3><p>While DFS-based schedules (like 1F1B and its variants) dominate in practice, they come with a key limitation. They require a large number of microbatches to fully utilize the pipeline‚Äîwhich isn‚Äôt always feasible.</p><p>To address this, <a href=https://arxiv.org/pdf/2211.05953>breath-first PP paper</a> proposed an alternative:</p><ul><li>Use similar virtual pipeline stages as in interleaved but use BFS (instead of DFS) for execution</li><li>Combine it with FSDP and activation checkpointing to reduce peak memory usage.</li></ul><p><img alt="BFS looping schedule" loading=lazy src=/posts/pipeline-parallelism-demystified/looped_bfs-noted.png>
This combo turns out to be helpful in small batch size settings, where DFS would otherwise stall or waste resources due to insufficient microbatches.</p><h1 id=common-questions>Common Questions<a hidden class=anchor aria-hidden=true href=#common-questions>#</a></h1><p>Q: What if a single layer can‚Äôt fit on a GPU?</p><p>A: Then PP isn‚Äôt enough. You‚Äôll need tensor parallelism (TP), where multiple GPUs jointly run one operation (e.g. matrix multiply).</p><p>Q: Why is scheduling so hard?</p><p>A: Because you‚Äôre solving a data dependency scheduling problem under communication and compute constraints. Minimizing idle time while respecting order is‚Ä¶ hard.</p><p>Q: Why not automate it?</p><p>A: Great idea! In fact, many teams are trying. But a proper scheduler needs to know model structure, compute cost, activation sizes, hardware bandwidths, and more. It‚Äôs a compiler problem‚Äîbut compilers for distributed training are still in early days.</p><h1 id=closing-thoughts>Closing Thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h1><p>I don&rsquo;t really expect this post to be as detailed as the papers so I&rsquo;ve intentionally excluded all the formulas here. But hopefully it gives you enough intuition to navigate them with more confidence.</p><p>At its core, pipeline parallelism is a simple idea. But the implementation is a surprisingly deep rabbit hole.</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on x" href="https://x.com/intent/tweet/?text=Pipeline%20Parallelism%20Demystified&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f&amp;title=Pipeline%20Parallelism%20Demystified&amp;summary=Pipeline%20Parallelism%20Demystified&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f&title=Pipeline%20Parallelism%20Demystified"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on whatsapp" href="https://api.whatsapp.com/send?text=Pipeline%20Parallelism%20Demystified%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on telegram" href="https://telegram.me/share/url?text=Pipeline%20Parallelism%20Demystified&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Pipeline Parallelism Demystified on ycombinator" href="https://news.ycombinator.com/submitlink?t=Pipeline%20Parallelism%20Demystified&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpipeline-parallelism-demystified%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>My New Hugo Site</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>