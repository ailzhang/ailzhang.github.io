<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Visualizing Parallelism in Transformer | Ailing's Blog</title><meta name=keywords content="Distributed Training,Transformer,Parallelism,ML Systems"><meta name=description content="A visual guide to the 'alphabet soup' of Tensor, Sequence, Context, and Expert Parallelism."><meta name=author content="Ailing Zhang"><link rel=canonical href=http://localhost:1313/posts/distributed-compute-in-transformer/><meta name=google-site-verification content="sn7RGeAvw0abr_XmUEQp40zXyDQPl2uCFo0e8NLddng"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/distributed-compute-in-transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/distributed-compute-in-transformer/"><meta property="og:site_name" content="Ailing's Blog"><meta property="og:title" content="Visualizing Parallelism in Transformer"><meta property="og:description" content="A visual guide to the 'alphabet soup' of Tensor, Sequence, Context, and Expert Parallelism."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-19T22:12:33-08:00"><meta property="article:modified_time" content="2026-01-19T22:12:33-08:00"><meta property="article:tag" content="Distributed Training"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Parallelism"><meta property="article:tag" content="ML Systems"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Visualizing Parallelism in Transformer"><meta name=twitter:description content="A visual guide to the 'alphabet soup' of Tensor, Sequence, Context, and Expert Parallelism."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Visualizing Parallelism in Transformer","item":"http://localhost:1313/posts/distributed-compute-in-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Visualizing Parallelism in Transformer","name":"Visualizing Parallelism in Transformer","description":"A visual guide to the 'alphabet soup' of Tensor, Sequence, Context, and Expert Parallelism.","keywords":["Distributed Training","Transformer","Parallelism","ML Systems"],"articleBody":"Simplicity buried in Abstractions I’ve always loved the “Transformer Accounting” diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.\nBut as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the ‘alphabet soup’ of N-D parallelism (DP, TP, SP, CP, EP).\nHere is the irony: The core ideas behind these parallelisms are actually fundamentally easy. Conceptually, we are just decomposing a global tensor operation into local tensor compute chunks connected by communication collectives. It’s like an assembly line: instead of one worker building the whole car, we have a line of workers (GPUs) passing parts (tensors) back and forth.\nBut in practice, this simplicity gets buried under layers of dense abstractions and “spaghetti” implementations in production codebases.\nI created the diagrams below to cut through that noise. I want to restore that intuition by visually mapping the dance between Compute and Collectives. The goal is to show exactly how the compute is sharded and when the synchronization happens: stripping away the implementation complexity to reveal the simple logic underneath.\nHow to Read Diagrams below The Golden Rule: To interpret the figures below, imagine you are sitting inside one single GPU.\nModel shape symbols To make the abstract symbols concrete, I’ve included the dimensions of Llama 3 70B as a reference.\nSymbol Definition Description Llama 3 70B B Batch Size Global batch size (sequences). Total ~4M tokens. 512 S Sequence Length Context window size. 8192 D Hidden Dim Width of the residual stream. 8192 V Vocab Size Total size of the tokenizer vocabulary. 128k F FFN Dim Expansion dimension in MLP. 28672 E Num. Experts Total experts (Llama 3 is dense). - C Capacity Max tokens per expert (MoE specific). - Parallel Configuration (The Sharding Strategy) These symbols represent the size of the process group used to shard a specific dimension.\nSymbol Definition Shards What? tp Tensor Parallel Weights (shards Heads in Attention, FFN Dim (F) in MLP) sp Sequence Parallel Activations (S) in Element-wise Ops cp Context Parallel Sequence (S) in Attention (QKV) ep Expert Parallel Experts (E) in MoE Layers vp Vocab Parallel Vocabulary (V) in Embeddings/Loss dp Data Parallel Batch (B) The “Local Shape” You will frequently see this specific shape entering and exiting the major blocks:\n[B/dp, S/(cp*sp), D]\nRead this literally from left to right:\nB/dp: “I only hold a fraction of the Batch.” (Data Parallelism) S/(cp*sp): “I only hold a tiny fragment of the Sequence.” (Context \u0026 Sequence Parallelism) D: “I hold the full Hidden Model vector.” The Visual Walkthrough Overview This diagram provides a high-level overview of layers of a Transformer model. Note that for the Feed-Forward Network (FFN) block, I cover both the Dense variant (standard MLP) and the Sparse variant (Mixture of Experts), as modern large-scale models frequently toggle between these designs.\nEmbeddings The Strategy: Vocab Parallel (VP) → Sequence Parallel (SP)\nThe Story: We start with input tokens that are sharded by the sequence dimension. The first challenge is the lookup. The embedding table for a model like Llama 3 is massive (128k rows * 8k dim ≈ 1GB of bf16 weights). We can’t replicate this on every GPU.\nInstead, we shard the vocabulary itself (Vocab Parallel). Each GPU holds a slice of the vocabulary. When we do a lookup, most tokens won’t be found on our local slice—they return zeros.\nThe Optimization (The ReduceScatter Trick): A naive approach would be to sum up all the partial lookups (AllReduce) and then split them again for the next layer. But that’s wasteful. Instead, we use a ReduceScatter. We sum the partial embeddings from the Vocab Parallel lookup and immediately scatter them into the Sequence Parallel dimension. This cuts communication overhead significantly right at the start.\nWhy this matters: Without VP, the embedding layer alone can consume gigabytes of redundant memory. VP spreads that cost, while the ReduceScatter ensures we don’t pay a double penalty for communication.\nAttention Here we see the complex interplay of different sequence strategies colliding in one block.\nThe Strategy: Tensor Parallel (TP) + Sequence Parallel (SP) + Context Parallel (CP)\nThe Story:\nEntry (Rebuild the Sequence): We enter this block with our sequence chopped up into tiny pieces (Sequence Parallel). But to compute standard Self-Attention, we need to project Q, K, and V. We trigger an AllGather(sp) to temporarily reconstruct the local sequence segment. The Attention Core (CP): Now things get interesting. We use Context Parallelism here. While the Query (Q) stays local, the Keys (K) and Values (V) are scattered across the CP ring. We have to pass them around the ring (often overlapping this communication with the computation) so every token can attend to every other token. Exit (The TP/SP Handoff): After attention, we project the output. This projection is Row Parallel, meaning it results in partial sums sharded by the hidden dimension. We use the ReduceScatter trick again: we sum the partial results from TP and simultaneously reshard them back into the memory-efficient SP-sharded shape. MLP (Feed Forward) The Strategy: Tensor Parallel (TP) + Sequence Parallel (SP)\nThe Story: The MLP block is the heavy lifter of compute.\nEntry: Just like Attention, we can’t process the sequence while it’s sharded. We fire an AllGather(sp) to rebuild the sequence. The Sandwich: The MLP is a “sandwich” of linear layers. The first layer (Gate/Up projection) expands the hidden dimension (usually 4x). We slice this “column-wise” (Column Parallel). The second layer (Down projection) shrinks it back down, and we slice this “row-wise” (Row Parallel). Exit: The result of the Row Parallel layer is partial sums. We run a ReduceScatter to sum them up and instantly return to our sequence-sharded state. Mixture of Experts (MoE) This is the most complex diagram. We aren’t just sharding tensors; we are actively routing tokens to different devices.\nThe Strategy: Expert Parallel (EP) + Tensor Parallel (TP)\nThe Story:\nThe Dispatch: Tokens need to go to their assigned experts. Since experts live on different GPUs (Expert Parallel), we use an AllToAll collective—literally “shuffle everything to everywhere.” Inner Parallelism: Once the tokens arrive at their expert, the computation looks just like a standard MLP. Interestingly, inside this expert, we can still apply Tensor Parallelism if the expert itself is too big! The Return: After the experts process the tokens, we have to send them back. We first ReduceScatter any inner Tensor Parallelism results, and then perform a final AllToAll to route the tokens back to their original sequence position. Why this matters: MoE allows us to scale parameters to the trillions without exploding the compute cost, but it turns the network interconnect into a bottleneck. The efficiency of that AllToAll shuffle often determines the training speed.\nLoss Calculating Cross Entropy when the vocab is sharded (V/vp) is non-trivial. You cannot simply take a softmax because the denominator requires a global sum.\nThe Strategy: Vocab Parallel (VP)\nThe Story:\nLogits: We compute the final logits using a Local Unembed (Vocab Parallel). This gives us logits that are sliced by the vocabulary dimension. Online Softmax: To compute softmax, we need the max (for stability) and the sum (for the denominator). We run AllReduce(max) and AllReduce(sum) across the vocab group to get these global values. Target Masking: This is the tricky part. The ground-truth target label for a token exists on one specific GPU, but the logits are scattered everywhere. We mask the non-local logits and use an AllReduce(sum) to broadcast the correct target logit to everyone in the group, allowing the loss calculation to proceed. Closing Thoughts These diagrams represent a snapshot of a common, representative setup found in modern frameworks like Megatron-Core or NVIDIA NeMo. It is not the only way to do it.\nThe challenge in infrastructure is making these parallelisms easy to understand, debug, and compose without sacrificing the performant timing of those critical collectives. Hopefully, these visual maps help you navigate the territory.\nA Note on FSDP: You might notice FSDP (Fully Sharded Data Parallel) is intentionally left out. This is because FSDP is fundamentally a memory optimization (sharding weights and optimizer states at rest) rather than a computation parallelism. FSDP gathers the full weights just-in-time for the forward/backward pass, meaning it does not affect the local tensor shapes that participate in the actual compute operations. I might cover FSDP and its interaction with these parallelisms in a follow-up post.\n","wordCount":"1401","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2026-01-19T22:12:33-08:00","dateModified":"2026-01-19T22:12:33-08:00","author":{"@type":"Person","name":"Ailing Zhang"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/distributed-compute-in-transformer/"},"publisher":{"@type":"Organization","name":"Ailing's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Visualizing Parallelism in Transformer
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentColor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-description>A visual guide to the 'alphabet soup' of Tensor, Sequence, Context, and Expert Parallelism.</div><div class=post-meta><span title='2026-01-19 22:12:33 -0800 PST'>January 19, 2026</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1401 words&nbsp;·&nbsp;Ailing Zhang&nbsp;|&nbsp;<a href=https://github.com/ailzhang/ailzhang.github.io/tree/main/content/posts/distributed-compute-in-transformer/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=simplicity-buried-in-abstractions><strong>Simplicity buried in Abstractions</strong><a hidden class=anchor aria-hidden=true href=#simplicity-buried-in-abstractions>#</a></h2><p>I&rsquo;ve always loved the <a href=https://jax-ml.github.io/scaling-book/transformers/>&ldquo;Transformer Accounting&rdquo;</a> diagram from the JAX Scaling Book. It did a brilliant job of making the tensor shapes of a Transformer intuitive on a single device.</p><p>But as we scale up, the complexity shifts. We stop worrying about just matrix dimensions and start worrying about the &lsquo;alphabet soup&rsquo; of N-D parallelism (<strong>DP, TP, SP, CP, EP</strong>).</p><p>Here is the irony: The core ideas behind these parallelisms are actually <strong>fundamentally easy</strong>. Conceptually, we are just decomposing a global tensor operation into local tensor compute chunks connected by communication collectives. It&rsquo;s like an assembly line: instead of one worker building the whole car, we have a line of workers (GPUs) passing parts (tensors) back and forth.</p><p>But in practice, this simplicity gets buried under layers of dense abstractions and &ldquo;spaghetti&rdquo; implementations in production codebases.</p><p>I created the diagrams below to cut through that noise. I want to restore that intuition by visually mapping the dance between <strong>Compute</strong> and <strong>Collectives</strong>. The goal is to show exactly <em>how</em> the compute is sharded and <em>when</em> the synchronization happens: stripping away the implementation complexity to reveal the simple logic underneath.</p><h2 id=how-to-read-diagrams-below><strong>How to Read Diagrams below</strong><a hidden class=anchor aria-hidden=true href=#how-to-read-diagrams-below>#</a></h2><p><strong>The Golden Rule:</strong> To interpret the figures below, imagine you are sitting inside one single GPU.</p><h3 id=model-shape-symbols>Model shape symbols<a hidden class=anchor aria-hidden=true href=#model-shape-symbols>#</a></h3><p>To make the abstract symbols concrete, I&rsquo;ve included the dimensions of <strong><a href=https://ai.meta.com/blog/meta-llama-3/>Llama 3 70B</a></strong> as a reference.</p><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Definition</th><th style=text-align:left>Description</th><th style=text-align:left>Llama 3 70B</th></tr></thead><tbody><tr><td style=text-align:left><strong>B</strong></td><td style=text-align:left>Batch Size</td><td style=text-align:left>Global batch size (<strong>sequences</strong>). Total ~4M tokens.</td><td style=text-align:left>512</td></tr><tr><td style=text-align:left><strong>S</strong></td><td style=text-align:left>Sequence Length</td><td style=text-align:left>Context window size.</td><td style=text-align:left>8192</td></tr><tr><td style=text-align:left><strong>D</strong></td><td style=text-align:left>Hidden Dim</td><td style=text-align:left>Width of the residual stream.</td><td style=text-align:left>8192</td></tr><tr><td style=text-align:left><strong>V</strong></td><td style=text-align:left>Vocab Size</td><td style=text-align:left>Total size of the tokenizer vocabulary.</td><td style=text-align:left>128k</td></tr><tr><td style=text-align:left><strong>F</strong></td><td style=text-align:left>FFN Dim</td><td style=text-align:left>Expansion dimension in MLP.</td><td style=text-align:left>28672</td></tr><tr><td style=text-align:left><strong>E</strong></td><td style=text-align:left>Num. Experts</td><td style=text-align:left>Total experts (Llama 3 is dense).</td><td style=text-align:left>-</td></tr><tr><td style=text-align:left><strong>C</strong></td><td style=text-align:left>Capacity</td><td style=text-align:left>Max tokens per expert (MoE specific).</td><td style=text-align:left>-</td></tr></tbody></table><h3 id=parallel-configuration-the-sharding-strategy>Parallel Configuration (The Sharding Strategy)<a hidden class=anchor aria-hidden=true href=#parallel-configuration-the-sharding-strategy>#</a></h3><p>These symbols represent the <strong>size</strong> of the process group used to shard a specific dimension.</p><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Definition</th><th style=text-align:left>Shards What?</th></tr></thead><tbody><tr><td style=text-align:left><strong>tp</strong></td><td style=text-align:left>Tensor Parallel</td><td style=text-align:left>Weights (shards <strong>Heads</strong> in Attention, <strong>FFN Dim (F)</strong> in MLP)</td></tr><tr><td style=text-align:left><strong>sp</strong></td><td style=text-align:left>Sequence Parallel</td><td style=text-align:left>Activations (S) in Element-wise Ops</td></tr><tr><td style=text-align:left><strong>cp</strong></td><td style=text-align:left>Context Parallel</td><td style=text-align:left>Sequence (S) in Attention (QKV)</td></tr><tr><td style=text-align:left><strong>ep</strong></td><td style=text-align:left>Expert Parallel</td><td style=text-align:left>Experts (E) in MoE Layers</td></tr><tr><td style=text-align:left><strong>vp</strong></td><td style=text-align:left>Vocab Parallel</td><td style=text-align:left>Vocabulary (V) in Embeddings/Loss</td></tr><tr><td style=text-align:left><strong>dp</strong></td><td style=text-align:left>Data Parallel</td><td style=text-align:left>Batch (B)</td></tr></tbody></table><h3 id=the-local-shape>The &ldquo;Local Shape&rdquo;<a hidden class=anchor aria-hidden=true href=#the-local-shape>#</a></h3><p>You will frequently see this specific shape entering and exiting the major blocks:</p><p><code>[B/dp, S/(cp*sp), D]</code></p><p>Read this literally from left to right:</p><ol><li><strong><code>B/dp</code></strong>: &ldquo;I only hold a fraction of the Batch.&rdquo; (Data Parallelism)</li><li><strong><code>S/(cp*sp)</code></strong>: &ldquo;I only hold a tiny fragment of the Sequence.&rdquo; (Context & Sequence Parallelism)</li><li><strong><code>D</code></strong>: &ldquo;I hold the <strong>full</strong> Hidden Model vector.&rdquo;</li></ol><hr><h2 id=the-visual-walkthrough><strong>The Visual Walkthrough</strong><a hidden class=anchor aria-hidden=true href=#the-visual-walkthrough>#</a></h2><h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3><p>This diagram provides a high-level overview of layers of a Transformer model. Note that for the Feed-Forward Network (FFN) block, I cover both the <strong>Dense</strong> variant (standard MLP) and the <strong>Sparse</strong> variant (Mixture of Experts), as modern large-scale models frequently toggle between these designs.</p><figure class=align-center><img loading=lazy src=./overview.svg#center width=400px></figure><hr><h3 id=embeddings><strong>Embeddings</strong><a hidden class=anchor aria-hidden=true href=#embeddings>#</a></h3><figure class=align-center><img loading=lazy src=./emb_parallel.svg#center width=400px></figure><p><strong>The Strategy:</strong> Vocab Parallel (VP) → Sequence Parallel (SP)</p><p><strong>The Story:</strong>
We start with input tokens that are sharded by the sequence dimension. The first challenge is the lookup. The embedding table for a model like Llama 3 is massive (128k rows * 8k dim ≈ 1GB of bf16 weights). We can&rsquo;t replicate this on every GPU.</p><p>Instead, we shard the vocabulary itself (Vocab Parallel). Each GPU holds a slice of the vocabulary. When we do a lookup, most tokens won&rsquo;t be found on our local slice—they return zeros.</p><p><strong>The Optimization (The ReduceScatter Trick):</strong>
A naive approach would be to sum up all the partial lookups (AllReduce) and then split them again for the next layer. But that&rsquo;s wasteful. Instead, we use a <strong>ReduceScatter</strong>. We sum the partial embeddings from the Vocab Parallel lookup and <em>immediately</em> scatter them into the Sequence Parallel dimension. This cuts communication overhead significantly right at the start.</p><blockquote><p><strong>Why this matters:</strong> Without VP, the embedding layer alone can consume gigabytes of redundant memory. VP spreads that cost, while the ReduceScatter ensures we don&rsquo;t pay a double penalty for communication.</p></blockquote><hr><h3 id=attention><strong>Attention</strong><a hidden class=anchor aria-hidden=true href=#attention>#</a></h3><p>Here we see the complex interplay of different sequence strategies colliding in one block.</p><figure class=align-center><img loading=lazy src=./attn_parallel.svg#center width=400px></figure><p><strong>The Strategy:</strong> Tensor Parallel (TP) + Sequence Parallel (SP) + Context Parallel (CP)</p><p><strong>The Story:</strong></p><ol><li><strong>Entry (Rebuild the Sequence):</strong> We enter this block with our sequence chopped up into tiny pieces (Sequence Parallel). But to compute standard Self-Attention, we need to project Q, K, and V. We trigger an <code>AllGather(sp)</code> to temporarily reconstruct the local sequence segment.</li><li><strong>The Attention Core (CP):</strong> Now things get interesting. We use Context Parallelism here. While the Query (Q) stays local, the Keys (K) and Values (V) are scattered across the CP ring. We have to pass them around the ring (often overlapping this communication with the computation) so every token can attend to every other token.</li><li><strong>Exit (The TP/SP Handoff):</strong> After attention, we project the output. This projection is Row Parallel, meaning it results in partial sums sharded by the hidden dimension. We use the ReduceScatter trick again: we sum the partial results from TP and simultaneously reshard them back into the memory-efficient SP-sharded shape.</li></ol><hr><h3 id=mlp-feed-forward><strong>MLP (Feed Forward)</strong><a hidden class=anchor aria-hidden=true href=#mlp-feed-forward>#</a></h3><figure class=align-center><img loading=lazy src=./mlp_parallel.svg#center width=400px></figure><p><strong>The Strategy:</strong> Tensor Parallel (TP) + Sequence Parallel (SP)</p><p><strong>The Story:</strong>
The MLP block is the heavy lifter of compute.</p><ol><li><strong>Entry:</strong> Just like Attention, we can&rsquo;t process the sequence while it&rsquo;s sharded. We fire an <code>AllGather(sp)</code> to rebuild the sequence.</li><li><strong>The Sandwich:</strong> The MLP is a &ldquo;sandwich&rdquo; of linear layers. The first layer (Gate/Up projection) expands the hidden dimension (usually 4x). We slice this &ldquo;column-wise&rdquo; (Column Parallel). The second layer (Down projection) shrinks it back down, and we slice this &ldquo;row-wise&rdquo; (Row Parallel).</li><li><strong>Exit:</strong> The result of the Row Parallel layer is partial sums. We run a <code>ReduceScatter</code> to sum them up and instantly return to our sequence-sharded state.</li></ol><hr><h3 id=mixture-of-experts-moe><strong>Mixture of Experts (MoE)</strong><a hidden class=anchor aria-hidden=true href=#mixture-of-experts-moe>#</a></h3><p>This is the most complex diagram. We aren&rsquo;t just sharding tensors; we are actively routing tokens to different devices.</p><figure class=align-center><img loading=lazy src=./moe_parallel.svg#center width=400px></figure><p><strong>The Strategy:</strong> Expert Parallel (EP) + Tensor Parallel (TP)</p><p><strong>The Story:</strong></p><ol><li><strong>The Dispatch:</strong> Tokens need to go to their assigned experts. Since experts live on different GPUs (Expert Parallel), we use an <code>AllToAll</code> collective—literally &ldquo;shuffle everything to everywhere.&rdquo;</li><li><strong>Inner Parallelism:</strong> Once the tokens arrive at their expert, the computation looks just like a standard MLP. Interestingly, inside this expert, we can <em>still</em> apply Tensor Parallelism if the expert itself is too big!</li><li><strong>The Return:</strong> After the experts process the tokens, we have to send them back. We first <code>ReduceScatter</code> any inner Tensor Parallelism results, and then perform a final <code>AllToAll</code> to route the tokens back to their original sequence position.</li></ol><blockquote><p><strong>Why this matters:</strong> MoE allows us to scale parameters to the trillions without exploding the compute cost, but it turns the network interconnect into a bottleneck. The efficiency of that <code>AllToAll</code> shuffle often determines the training speed.</p></blockquote><hr><h3 id=loss><strong>Loss</strong><a hidden class=anchor aria-hidden=true href=#loss>#</a></h3><p>Calculating Cross Entropy when the vocab is sharded (V/vp) is non-trivial. You cannot simply take a softmax because the denominator requires a global sum.</p><figure class=align-center><img loading=lazy src=./loss_parallel.svg#center width=400px></figure><p><strong>The Strategy:</strong> Vocab Parallel (VP)</p><p><strong>The Story:</strong></p><ol><li><strong>Logits:</strong> We compute the final logits using a Local Unembed (Vocab Parallel). This gives us logits that are sliced by the vocabulary dimension.</li><li><strong>Online Softmax:</strong> To compute softmax, we need the max (for stability) and the sum (for the denominator). We run <code>AllReduce(max)</code> and <code>AllReduce(sum)</code> across the vocab group to get these global values.</li><li><strong>Target Masking:</strong> This is the tricky part. The ground-truth target label for a token exists on <em>one</em> specific GPU, but the logits are scattered everywhere. We mask the non-local logits and use an <code>AllReduce(sum)</code> to broadcast the correct target logit to everyone in the group, allowing the loss calculation to proceed.</li></ol><hr><h2 id=closing-thoughts><strong>Closing Thoughts</strong><a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>These diagrams represent a snapshot of a common, representative setup found in modern frameworks like <strong>Megatron-Core</strong> or <strong>NVIDIA NeMo</strong>. It is not the only way to do it.</p><p>The challenge in infrastructure is making these parallelisms easy to understand, debug, and compose without sacrificing the performant timing of those critical collectives. Hopefully, these visual maps help you navigate the territory.</p><p><strong>A Note on FSDP:</strong>
You might notice <strong>FSDP (Fully Sharded Data Parallel)</strong> is intentionally left out. This is because FSDP is fundamentally a memory optimization (sharding weights and optimizer states at rest) rather than a computation parallelism. FSDP gathers the full weights just-in-time for the forward/backward pass, meaning it does not affect the local tensor shapes that participate in the actual compute operations. I might cover FSDP and its interaction with these parallelisms in a follow-up post.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/distributed-training/>Distributed Training</a></li><li><a href=http://localhost:1313/tags/transformer/>Transformer</a></li><li><a href=http://localhost:1313/tags/parallelism/>Parallelism</a></li><li><a href=http://localhost:1313/tags/ml-systems/>ML Systems</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/pipeline-parallelism-demystified/><span class=title>Next »</span><br><span>Pipeline Parallelism Demystified</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on x" href="https://x.com/intent/tweet/?text=Visualizing%20Parallelism%20in%20Transformer&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f&amp;hashtags=DistributedTraining%2cTransformer%2cParallelism%2cMLSystems"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f&amp;title=Visualizing%20Parallelism%20in%20Transformer&amp;summary=Visualizing%20Parallelism%20in%20Transformer&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f&title=Visualizing%20Parallelism%20in%20Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on whatsapp" href="https://api.whatsapp.com/send?text=Visualizing%20Parallelism%20in%20Transformer%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on telegram" href="https://telegram.me/share/url?text=Visualizing%20Parallelism%20in%20Transformer&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing Parallelism in Transformer on ycombinator" href="https://news.ycombinator.com/submitlink?t=Visualizing%20Parallelism%20in%20Transformer&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fdistributed-compute-in-transformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Ailing's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>